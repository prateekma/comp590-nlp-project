{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-S6Sg64tUYo1QFefBLmt9svb-huWb_xS","timestamp":1681049947563},{"file_id":"1Ln8c9eOm1suILhlQo5MO_6MaUeZGWde6","timestamp":1678289740504},{"file_id":"12_VoGOF13pEuQlacO2RQ1vOVbmnWBrWS","timestamp":1678235612372}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":36,"metadata":{"id":"rpDwZqpnQ2iw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681787680440,"user_tz":240,"elapsed":8,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"33a85cbd-98e8-46b9-98b7-0601d00d19bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f0a446c3310>"]},"metadata":{},"execution_count":36}],"source":["import os\n","import sys\n","import time\n","import math\n","import argparse\n","from dataclasses import dataclass\n","from typing import List\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","torch.manual_seed(5190)"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EZhswgQ0d62Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681783794383,"user_tz":240,"elapsed":19995,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"e5b3cc55-c1b5-4eda-fb68-ed83d975676b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Read the text file\n","input_file_path = os.path.join('/content/drive/My Drive/', 'training_dataset.txt')\n","if not os.path.exists(input_file_path):\n","    print('File not found!')\n"],"metadata":{"id":"-QeF9ymlQ6ws","executionInfo":{"status":"ok","timestamp":1681783863917,"user_tz":240,"elapsed":140,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Examine the dataset\n","with open(input_file_path, 'r') as f:\n","    data = f.read()\n","print(f\"length of dataset in characters: {len(data):,}\")"],"metadata":{"id":"kWWDOd3ACfXK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681783867392,"user_tz":240,"elapsed":861,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"2d8834e1-90c5-4971-cd0d-90334d7f7069"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters: 52,780\n"]}]},{"cell_type":"code","source":["# get the vocabulary (i.e. the unique chars)\n","# chars should be a SORTED list of unique characters in the data object\n","\n","chars = list(set(data))\n","chars.sort()\n","\n","vocab_size = len(chars)\n","print(\"all the unique characters:\", ''.join(chars))\n","print(f\"vocab size: {vocab_size:,}\")"],"metadata":{"id":"jUA8Gt_-Cdmm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681783872146,"user_tz":240,"elapsed":123,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"a8ec2667-b59a-41e0-8a07-a079ba67c97e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["all the unique characters: \n"," !\"'(),-.2;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","vocab size: 65\n"]}]},{"cell_type":"code","source":["# a mapping from character to integer\n","ctoi = {s:i for i,s in enumerate(chars)}\n","\n","\n","# a mapping from integer to character\n","# that reverses the ctoi constructed above\n","\n","itoc = {i:s for i,s in enumerate(chars)}"],"metadata":{"id":"F4CQR0M-e40N","executionInfo":{"status":"ok","timestamp":1681783890783,"user_tz":240,"elapsed":407,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# encode the dataset\n","def encode(text):\n","  # input: a string\n","  # output: a list of integers that represents each character in the string\n","  return [ctoi[s] for s in text]\n","\n","\n","# decode the dataset\n","def decode(rep):\n","  # input:  a list of integers \n","  # output: a string that was represented by the input list of integers\n","\n","  str_list = [itoc[a] for a in rep]\n","  return ''.join(str_list)  "],"metadata":{"id":"g-4LMZRJgc1A","executionInfo":{"status":"ok","timestamp":1681783892403,"user_tz":240,"elapsed":273,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# create the train and test splits\n","n = len(data)\n","train_data = data[:int(n*0.9)]\n","val_data = data[int(n*0.9):]\n","\n","# encode both to integers\n","train_ids = torch.tensor(encode(train_data),dtype=torch.long)\n","val_ids = torch.tensor(encode(val_data),dtype=torch.long)"],"metadata":{"id":"jtpc1hbyhvyu","executionInfo":{"status":"ok","timestamp":1681783894324,"user_tz":240,"elapsed":282,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Define a block size, batch_size, and construct the batch\n","# Create the training/test datasets\n","# Recall that we want to take in the past block_size characeters (where we \n","# choose block_size) and output the next charcater\n","block_size = 16\n","\n","# Args: \n","#   split: set to train when want to use train_ids, set to val when want to use\n","#          val_ids\n","#   batch_size: the size of the batch for training\n","# Outputs:\n","#   x: training/val data, where each row is a list of previous character ids\n","#   y: training/val ground truth, where each entry is the ground-truth character\n","#      id for the current character\n","# \n","# In essence, we want to use the list of previous character ids ot predict the \n","# current character id\n","\n","# Note that the output x and y should be torch tensors. \n","\n","def get_batch(split,batch_size):\n","  data = train_ids if split=='train' else val_ids\n","  ix = torch.randint(len(data)-block_size,(batch_size,))\n","  x = torch.stack([data[i:i+block_size] for i in ix])\n","  y = torch.tensor([data[i+block_size] for i in ix])\n","  return x,y"],"metadata":{"id":"qDmjcZJIjKo2","executionInfo":{"status":"ok","timestamp":1681786657826,"user_tz":240,"elapsed":242,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# Initialize the embedding dictionary\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","torch.manual_seed(5190)\n","\n","n_embd = 80\n","n_embd2 = 256\n","\n","class MLP_neural_language(nn.Module):\n","    \"\"\"\n","    takes the previous block_size tokens, encodes them with a lookup table,\n","    concatenates the vectors and predicts the next token with an MLP.\n","    Reference:\n","    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n","    \"\"\"\n","\n","    def __init__(self, block_size,vocab_size,n_embd,n_embd2):\n","        super().__init__()\n","        self.block_size =block_size\n","        self.vocab_size =vocab_size\n","        self.wte = nn.Embedding(vocab_size,n_embd) \n","        self.mlp = nn.Sequential(\n","            nn.Linear(self.block_size *n_embd,n_embd2),\n","            nn.Tanh(),\n","            nn.Linear(n_embd2, self.vocab_size)\n","        )\n","\n","    def get_block_size(self):\n","        return self.block_size\n","\n","    def forward(self, inputs_idx, targets=None):\n","        inputs_embd = self.wte(inputs_idx)\n","        inputs_embd = inputs_embd.reshape(inputs_embd.shape[0],-1)\n","        logits = self.mlp(inputs_embd)\n","        # if we are given some desired targets also calculate the loss\n","        loss = None\n","        if targets is not None:\n","            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","\n","        return logits, loss\n","\n","\n","m = MLP_neural_language(block_size,vocab_size,n_embd,n_embd2)"],"metadata":{"id":"A8KmgsegmpyF","executionInfo":{"status":"ok","timestamp":1681787687020,"user_tz":240,"elapsed":155,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["# a simple training training loop\n","m = m.to('cuda')\n","batch_size = 4096\n","num_epochs = 30\n","num_batches = 256\n","optimizer = torch.optim.Adam(m.parameters(),lr=1e-3)\n","for i in range(num_epochs):\n","  epoch_loss = 0.0\n","  for steps in range(num_batches):\n","    optimizer.zero_grad(set_to_none=True)\n","\n","    # sample a batch of data\n","    xb,yb = get_batch('train', batch_size)\n","\n","    xb = xb.to('cuda')\n","    yb = yb.to('cuda')\n","\n","    # evaluate the loss\n","    logits, loss =  m(xb, yb)\n","\n","    loss.backward()\n","    \n","    optimizer.step()\n","\n","    epoch_loss += loss.item()\n","\n","  print(epoch_loss/num_batches)"],"metadata":{"id":"CGG_irmT9joZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681788496199,"user_tz":240,"elapsed":779697,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"d639316d-0e21-43a9-99a5-503e4db0ceb6"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["1.5562942894175649\n","0.31910447680274956\n","0.06418634732835926\n","0.025690279879199807\n","0.01433028594692587\n","0.009261041295758332\n","0.006516140361782163\n","0.004856060912970861\n","0.003729683976416709\n","0.0029881180789743667\n","0.002427669969620183\n","0.002013767864809779\n","0.0017262325422962022\n","0.0014298174492068938\n","0.0012763875752170861\n","0.0011215933318453608\n","0.0010130664668395184\n","0.0008864019132488465\n","0.000802357483621563\n","0.02412743557624708\n","0.008847959074500977\n","0.0012301118856612447\n","0.0009018886726153141\n","0.0007987368994690769\n","0.0006858012859538576\n","0.0006294185532169649\n","0.0005732847138801844\n","0.0005094436909871547\n","0.00048298538530389123\n","0.00047452504338707513\n"]}]},{"cell_type":"code","source":["# Generation\n","# We use the first block_size number of characters in the val set to start \n","# generating Shakespear-styled writings. \n","\n","m = m.to('cpu')\n","limit = 2000\n","# the first block_size number of characters in val_ids\n","context = val_ids[:block_size].tolist()\n","generation = context\n","\n","for i in range(limit):\n","  # Please finish this for loop for generation at test time given the initial \n","  # context. Note context is of data type list, but the trained model m \n","  # expects torch tensor objects. Just a heads up. \n","  context_tensor = torch.tensor(context, dtype=torch.long).unsqueeze(0)\n","  \n","  i_embd = m.wte(context_tensor)\n","  i_embd = i_embd.reshape(i_embd.shape[0], -1)\n","  logits = m.mlp(i_embd)\n","  next_token = torch.multinomial(F.softmax(logits, dim=1).squeeze(0), 1).item()\n","  generation.append(next_token)\n","  context = generation[-block_size:]\n"],"metadata":{"id":"tS4rshzWS-Cm","executionInfo":{"status":"ok","timestamp":1681788611530,"user_tz":240,"elapsed":1097,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["# Decode the generation and see what the AI came up with :)\n","print(decode(generation))"],"metadata":{"id":"YdvDnz7FUvUg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681788612628,"user_tz":240,"elapsed":6,"user":{"displayName":"Prateek Machiraju","userId":"17894039480283150405"}},"outputId":"352e6895-b88c-4a29-86f4-d6949f79da1a"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["PIO\n","\n","It sounds like Han.\n","\n","LANDO\n","\n","(in the gilll. seru!\n","\n","LANDO\n","\n","Wh, wha beailes.\n","\n","Thed Pre't sher.\n","\n","REETSON\n","\n","Come in!  Lake a chat ore of this\n","\n","pithe pederays woll pearing andreq?\n","\n","HAN\n","\n","Youh, into That boply nowllly.\n","\n","Ches I mand that'm  nit hemp,rie.\n","\n","HAN\n","\n","Prenger you got het your stoung\n","\n","You be af, shute\n","\n","This I din't no we\n","\n","mige powe the go for beed?\n","\n","LEIA\n","\n","(takrend)\n","\n","Okhan tra ais on!  We tuste quatong the kto\n","\n","that me to stom the Ragy that ife\n","\n","here in thit palind got to peat're fou donto the\n","\n","mige ang the erve.  Monot manter\n","\n","for the mandread wian whave are tareera\n","\n","sight meno ght peacr.\n","\n","LEIA\n","\n","You po as unt ingat.  We're flint hes itt ourd\n","\n","inet the fore weread I'm know.\n","\n","YODA\n","\n","Sou dout hime to falle can the\n","\n","fack in are wirle, IM\n","\n","the sereca dount wo lhit tha gore.\n","\n","THREEPIO\n","\n","Well, in the stams fecl is.\n","\n","HAN\n","\n","Mome Jes are thing, I mone herm?\n","\n","ANDO\n","\n","Com ion, I winl ie tiat in minlt\n","\n","ave there it lo do the\n","\n","areatere fore.  now manter.\n","\n","LEIA\n","\n","HAN\n","\n","Year?\n","\n","YODA\n","\n","We't s otiy u tan wit hit\n","\n","arkeng be ale the dof cou the gon\n","\n","REET\n","\n","(saboun ans mptreadinghat hese.\n","\n","LUKE\n","\n","(outhang to noks!\n","\n","HAN\n","\n","Sher oo mist you a doo the vers\n","\n","mwais on thow reatl...  I'm gring ing\n","\n","with abeat oning too..  I mith this\n","\n","ctome tanter this.  I con's the\n","\n","pays beding to ste fir nous\n","\n","LUKE\n","\n","Alt Anop, yerezen goo ffrie to eld\n","\n","tie top of thim\n","\n","tre too ghe here for dowing to\n","\n","Stlout rinke poong to the firen\n","\n","qulle don't thes are upd I'm ngert\n","\n","is it dit oume foren there sile\n","\n","tounge coof the the chissa\n","\n","That.  I'm gof the thr Dodking\n","\n","cundowh.  I winntt nowe tore.\n","\n","Ohere's fere yout he the rost anct\n","\n","creine.\n","\n","LEIA\n","\n","I can'te lo bo now wline, lame\n","\n","master.\n","\n","Stor akingtore thing sire cimus\n","\n","HAN\n","\n","Tachers)\n","\n","Whal  ou to toreance\n","\n","fill you?\n","\n","Mom mon ere aring thit to thow\n","\n","nd, not hou.\n","\n","HAN\n","\n","I'm no too gon.\n","\n","LUKE\n","\n","(into comlink)\n","\n","We're going to the Dagobah system.\n","\n","LUKE\n","\n","(into comlink)\n","\n","Yes, Artoo?\n","\n","LUKE\n","\n","(into comlink, chuckling)\n","\n","That's all right.  I'd like to\n","\n","keep it on manual control for a\n","\n","while.\n","\n","HAN\n","\n","(harried)\n","\n","I saw them! \n"]}]}]}